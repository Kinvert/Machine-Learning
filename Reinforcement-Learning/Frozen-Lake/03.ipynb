{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858d4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 01:36:36.652484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:36.665551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:36.666283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:36.667537: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 01:36:36.667927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:36.668355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:36.668764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:37.113654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:37.114270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:37.114666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 01:36:37.115035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 0 epsilon=0.9 j=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 01:36:37.768898: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 epsilon=0.855 j=\n",
      "EPISODE 2 epsilon=0.8122499999999999 j=\n",
      "EPISODE 3 epsilon=0.7716374999999999 j=\n",
      "EPISODE 4 epsilon=0.7330556249999999 j=\n",
      "EPISODE 5 epsilon=0.6964028437499998 j=\n",
      "EPISODE 6 epsilon=0.6615827015624998 j=\n",
      "EPISODE 7 epsilon=0.6285035664843748 j=\n",
      "EPISODE 8 epsilon=0.597078388160156 j=\n",
      "EPISODE 9 epsilon=0.5672244687521482 j=\n",
      "EPISODE 10 epsilon=0.5388632453145408 j=\n",
      "EPISODE 11 epsilon=0.5119200830488138 j=\n",
      "EPISODE 12 epsilon=0.486324078896373 j=\n",
      "EPISODE 13 epsilon=0.4620078749515544 j=\n",
      "EPISODE 14 epsilon=0.43890748120397666 j=\n",
      "EPISODE 15 epsilon=0.4169621071437778 j=\n",
      "EPISODE 16 epsilon=0.3961140017865889 j=\n",
      "EPISODE 17 epsilon=0.37630830169725943 j=\n",
      "EPISODE 18 epsilon=0.3574928866123964 j=\n",
      "EPISODE 19 epsilon=0.33961824228177656 j=\n",
      "EPISODE 20 epsilon=0.3226373301676877 j=\n",
      "EPISODE 21 epsilon=0.3065054636593033 j=\n",
      "EPISODE 22 epsilon=0.29118019047633814 j=\n",
      "EPISODE 23 epsilon=0.2766211809525212 j=\n",
      "EPISODE 24 epsilon=0.26279012190489515 j=\n",
      "EPISODE 25 epsilon=0.24965061580965037 j=\n",
      "EPISODE 26 epsilon=0.23716808501916783 j=\n",
      "EPISODE 27 epsilon=0.22530968076820942 j=\n",
      "EPISODE 28 epsilon=0.21404419672979894 j=\n",
      "EPISODE 29 epsilon=0.20334198689330898 j=\n",
      "EPISODE 30 epsilon=0.19317488754864354 j=\n",
      "EPISODE 31 epsilon=0.18351614317121134 j=\n",
      "EPISODE 32 epsilon=0.17434033601265078 j=\n",
      "EPISODE 33 epsilon=0.16562331921201823 j=\n",
      "EPISODE 34 epsilon=0.15734215325141732 j=\n",
      "EPISODE 35 epsilon=0.14947504558884644 j=\n",
      "EPISODE 36 epsilon=0.14200129330940411 j=\n",
      "EPISODE 37 epsilon=0.1349012286439339 j=\n",
      "EPISODE 38 epsilon=0.1281561672117372 j=\n",
      "EPISODE 39 epsilon=0.12174835885115033 j=\n",
      "EPISODE 40 epsilon=0.11566094090859282 j=\n",
      "EPISODE 41 epsilon=0.10987789386316317 j=\n",
      "EPISODE 42 epsilon=0.104383999170005 j=\n",
      "EPISODE 43 epsilon=0.09916479921150474 j=\n",
      "EPISODE 44 epsilon=0.0942065592509295 j=\n",
      "EPISODE 45 epsilon=0.08949623128838302 j=\n",
      "EPISODE 46 epsilon=0.08502141972396386 j=\n",
      "EPISODE 47 epsilon=0.08077034873776566 j=\n",
      "EPISODE 48 epsilon=0.07673183130087738 j=\n",
      "EPISODE 49 epsilon=0.0728952397358335 j=\n",
      "EPISODE 50 epsilon=0.06925047774904183 j=\n",
      "EPISODE 51 epsilon=0.06578795386158974 j=\n",
      "EPISODE 52 epsilon=0.06249855616851025 j=\n",
      "EPISODE 53 epsilon=0.05937362836008474 j=\n",
      "EPISODE 54 epsilon=0.0564049469420805 j=\n",
      "EPISODE 55 epsilon=0.05358469959497647 j=\n",
      "EPISODE 56 epsilon=0.050905464615227644 j=\n",
      "EPISODE 57 epsilon=0.04836019138446626 j=\n",
      "EPISODE 58 epsilon=0.04594218181524295 j=\n",
      "EPISODE 59 epsilon=0.0436450727244808 j=\n",
      "EPISODE 60 epsilon=0.04146281908825676 j=\n",
      "EPISODE 61 epsilon=0.03938967813384392 j=\n",
      "EPISODE 62 epsilon=0.037420194227151725 j=\n",
      "EPISODE 63 epsilon=0.035549184515794134 j=\n",
      "EPISODE 64 epsilon=0.033771725290004426 j=\n",
      "EPISODE 65 epsilon=0.0320831390255042 j=\n",
      "EPISODE 66 epsilon=0.03047898207422899 j=\n",
      "EPISODE 67 epsilon=0.02895503297051754 j=\n",
      "EPISODE 68 epsilon=0.02750728132199166 j=\n",
      "EPISODE 69 epsilon=0.026131917255892076 j=\n",
      "EPISODE 70 epsilon=0.024825321393097472 j=\n",
      "EPISODE 71 epsilon=0.0235840553234426 j=\n",
      "EPISODE 72 epsilon=0.02240485255727047 j=\n",
      "EPISODE 73 epsilon=0.021284609929406943 j=\n",
      "EPISODE 74 epsilon=0.020220379432936596 j=\n",
      "EPISODE 75 epsilon=0.019209360461289765 j=\n",
      "EPISODE 76 epsilon=0.018248892438225275 j=\n",
      "EPISODE 77 epsilon=0.01733644781631401 j=\n",
      "EPISODE 78 epsilon=0.01646962542549831 j=\n",
      "EPISODE 79 epsilon=0.01564614415422339 j=\n",
      "EPISODE 80 epsilon=0.014863836946512221 j=\n",
      "EPISODE 81 epsilon=0.01412064509918661 j=\n",
      "EPISODE 82 epsilon=0.013414612844227278 j=\n",
      "EPISODE 83 epsilon=0.012743882202015912 j=\n",
      "EPISODE 84 epsilon=0.012106688091915117 j=\n",
      "EPISODE 85 epsilon=0.01150135368731936 j=\n",
      "EPISODE 86 epsilon=0.010926286002953391 j=\n",
      "EPISODE 87 epsilon=0.010379971702805722 j=\n",
      "EPISODE 88 epsilon=0.009860973117665435 j=\n",
      "EPISODE 89 epsilon=0.009367924461782163 j=\n",
      "EPISODE 90 epsilon=0.008899528238693053 j=\n",
      "EPISODE 91 epsilon=0.0084545518267584 j=\n",
      "EPISODE 92 epsilon=0.00803182423542048 j=\n",
      "EPISODE 93 epsilon=0.007630233023649456 j=\n",
      "EPISODE 94 epsilon=0.007248721372466983 j=\n",
      "EPISODE 95 epsilon=0.006886285303843633 j=\n",
      "EPISODE 96 epsilon=0.0065419710386514516 j=\n",
      "EPISODE 97 epsilon=0.0062148724867188785 j=\n",
      "EPISODE 98 epsilon=0.005904128862382934 j=\n",
      "EPISODE 99 epsilon=0.005608922419263787 j=\n",
      "0\n",
      "State: 0, Action: 3, Reward: -1.0, Done: False\n",
      "1\n",
      "State: 4, Action: 3, Reward: -1.0, Done: False\n",
      "2\n",
      "State: 8, Action: 3, Reward: -1.0, Done: False\n",
      "3\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "4\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "5\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "6\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "7\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "8\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "9\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "10\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "11\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "12\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "13\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "14\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "15\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "16\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "17\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "18\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "19\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "20\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "21\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "22\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "23\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "24\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "25\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "26\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "27\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "28\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "29\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "30\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "31\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "32\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "33\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "34\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "35\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "36\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "37\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "38\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "39\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "40\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "41\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "42\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "43\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "44\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "45\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "46\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "47\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "48\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "49\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "50\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "51\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "52\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "53\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "54\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "55\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "56\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "57\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "58\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "59\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "60\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "61\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "62\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "63\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "64\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "65\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "66\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "67\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "68\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "69\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "70\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "71\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "72\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "73\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "74\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "75\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "76\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "77\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "78\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "79\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "80\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "81\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "82\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "83\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "84\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "85\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "86\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "87\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "88\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "89\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "90\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "91\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "92\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "93\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "94\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "95\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "96\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "97\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "98\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n",
      "99\n",
      "State: 12, Action: 3, Reward: -1.0, Done: False\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the environment\n",
    "class FrozenLakeEnv:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.nS = size*size # Number of States\n",
    "        self.nA = 4 # Number of Actions\n",
    "        self.P = np.zeros((self.nS, self.nA, self.nS))\n",
    "        self.R = np.zeros((self.nS, self.nA, self.nS))\n",
    "        \n",
    "        # Define the game board\n",
    "        self.board = np.array([\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0]\n",
    "        ])\n",
    "        \n",
    "        # Define the starting and goal states\n",
    "        self.start_state = 0\n",
    "        self.goal_state = 15\n",
    "        \n",
    "        # Define the possible actions\n",
    "        self.actions = ['left', 'up', 'right', 'down']\n",
    "        \n",
    "        # Define the transition probabilities and rewards\n",
    "        for i in range(self.nS):\n",
    "            for j in range(self.nA):\n",
    "                for k in range(self.nS):\n",
    "                    if j == 0:  # Move left\n",
    "                        if i % self.size == 0:  # Wall on left\n",
    "                            self.P[i, j, i] = 1\n",
    "                            self.R[i, j, i] = -1\n",
    "                        else:\n",
    "                            self.P[i, j, i-1] = 1\n",
    "                            self.R[i, j, i-1] = -1\n",
    "                    elif j == 1:  # Move up\n",
    "                        if i < self.size:  # Wall on top\n",
    "                            self.P[i, j, i] = 1\n",
    "                            self.R[i, j, i] = -1\n",
    "                        else:\n",
    "                            self.P[i, j, i-self.size] = 1\n",
    "                            self.R[i, j, i-self.size] = -1\n",
    "                    elif j == 2:  # Move right\n",
    "                        if (i+1) % self.size == 0:  # Wall on right\n",
    "                            self.P[i, j, i] = 1\n",
    "                            self.R[i, j, i] = -1\n",
    "                        else:\n",
    "                            self.P[i, j, i+1] = 1\n",
    "                            self.R[i, j, i+1] = -1\n",
    "                    elif j == 3:  # Move down\n",
    "                        if i >= self.nS-self.size:  # Wall on bottom\n",
    "                            self.P[i, j, i] = 1\n",
    "                            self.R[i, j, i] = -1\n",
    "                        else:\n",
    "                            self.P[i, j, i+self.size] = 1\n",
    "                            self.R[i, j, i+self.size] = -1\n",
    "                        \n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(self.nS, p=self.P[self.current_state, action])\n",
    "        reward = self.R[self.current_state, action, next_state]\n",
    "        self.current_state = next_state\n",
    "        done = next_state == self.goal_state\n",
    "        #print('current={} goal={} done={}'.format(\n",
    "        #    self.current_state, self.goal_state, done))\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Define the agent\n",
    "class QAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = env.nS\n",
    "        self.nA = env.nA\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.9\n",
    "        \n",
    "        self.model = tf.keras.Sequential([\n",
    "            layers.Dense(16, input_shape=(1,), activation='relu'),\n",
    "            layers.Dense(self.nA, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                           loss='mean_squared_error')\n",
    "        \n",
    "    def get_action(self, state, epsilon=None):\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.nA)\n",
    "        else:\n",
    "            q_values = self.model(np.array([state]))\n",
    "            q_values = q_values.numpy()\n",
    "            return np.argmax(q_values)\n",
    "        \n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        q_values = self.model(np.array([state]))\n",
    "        try:\n",
    "            q_values = q_values.numpy()\n",
    "        except:\n",
    "            pass\n",
    "        if done:\n",
    "            q_values[0, action] = reward\n",
    "        else:\n",
    "            next_q_values = self.model(np.array([next_state]))\n",
    "            q_values[0, action] = reward + self.discount_factor * np.max(next_q_values)\n",
    "        #self.model.train_on_batch(np.array([state]), q_values)\n",
    "        self.model.fit(np.array([state]), q_values, verbose=0)\n",
    "\n",
    "# Define the game\n",
    "env = FrozenLakeEnv()\n",
    "agent = QAgent(env)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 100\n",
    "j = 0\n",
    "for i in range(num_episodes):\n",
    "    print('EPISODE {} epsilon={} j={}'.format(i, agent.epsilon, j))\n",
    "    env.reset()\n",
    "    state = env.start_state\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.train(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        j += 1\n",
    "    agent.epsilon *= 0.95\n",
    "\n",
    "# Test the agent\n",
    "env.reset()\n",
    "state = env.start_state\n",
    "done = False\n",
    "agent.epsilon = 0.0\n",
    "j = 0\n",
    "while not done and j < 100:\n",
    "    print(j)\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(f'State: {state}, Action: {action}, Reward: {reward}, Done: {done}')\n",
    "    state = next_state\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055d4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
